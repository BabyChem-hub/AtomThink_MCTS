import os
import sys
from os.path import dirname, exists, join, basename
import concurrent.futures
import argparse
import glob # Need this for finding scored files

current_dir = dirname(os.path.abspath(__file__))
sys.path.append(dirname(dirname(current_dir)))
sys.path.append(dirname(dirname(dirname(current_dir))))
print(sys.path)
print(os.getcwd())
from llamafactory.extras.logging import get_logger
from llamafactory.evaluation.utils.eval_utils import s0_merge_answers, s1_separate_n, s3_merge_save_answers, read_json, save_json, save_jsonl, read_jsonl, load_yaml, save_yaml
logger = get_logger(__name__)

def import_score_funcs(dataset_name):
    if "MathVista" in dataset_name:
        from llamafactory.evaluation.benchmark.mathvista import s2_extract_and_score, s4_show_scores
    elif "MathVerse" in dataset_name:
        from llamafactory.evaluation.benchmark.mathverse import s2_extract_and_score, s4_show_scores
    elif "MathVision" in dataset_name:
        from llamafactory.evaluation.benchmark.mathvision import s2_extract_and_score, s4_show_scores
    elif "HLE" == dataset_name:
        from llamafactory.evaluation.benchmark.hle import s2_extract_and_score, s4_show_scores
    else:
        logger.error(f"Unknown Evaluate Data: {dataset_name}")
        raise ValueError(f"Unknown dataset: {dataset_name}")
    return s2_extract_and_score, s4_show_scores
# --------------------------------------------------------------------

def _score_chunk_and_collect(s2_func, chunk_path):
    """Run chunk scoring in place and return the loaded results."""
    s2_func(chunk_path)
    return read_json(chunk_path)


def _score_chunk_worker(dataset_name, chunk_path):
    """Process pool worker wrapper to avoid pickling nested callables."""
    s2_func, _ = import_score_funcs(dataset_name)
    s2_func(chunk_path)
    return chunk_path


def scoring():

    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset_name", default='MathVista', type=str)
    parser.add_argument('--config', type=str, default=None, help="configs/train_full/test.yaml")
    parser.add_argument('--output_dir', type=str, default=None)
    parser.add_argument("--api_process", default=1, type=int, help="Number of processes for non-LLM judge scoring (ignored for LLM judge)") # Default changed to 1
    parser.add_argument("--tag", default="base_base", type=str)

    parser.add_argument("--score_with_llm", action="store_true", help="Use local LLM judge for scoring")
    parser.add_argument("--llm_model_path", type=str, default="/data1/xiangkun/MODELS/Qwen2.5-VL-7B-Instruct", help="Path to local LLM used for scoring")
    parser.add_argument("--llm_device_map", type=str, default="auto", help="Device map when loading the local LLM judge")
    parser.add_argument("--llm_dtype", type=str, default="bfloat16", help="Torch dtype for local LLM judge") # Changed back for 8bit handling below
    parser.add_argument("--llm_temperature", type=float, default=0.0, help="Sampling temperature for local LLM judge")
    parser.add_argument("--llm_max_new_tokens", type=int, default=32, help="Max new tokens generated by the local LLM judge")
    # --- 【代码修改】: 添加新的控制参数 ---
    parser.add_argument("--split_only", type=int, default=0, help="Only split data into N chunks and exit.")
    parser.add_argument("--process_chunk", type=int, default=-1, help="Only process chunk i (0-based) and exit.")
    parser.add_argument("--output_chunk_file", type=str, default=None, help="Output file path when using --process_chunk.")
    parser.add_argument("--merge_only", action="store_true", help="Only merge scored chunks and show final scores.")
    # --- 添加8bit加载选项，解决OOM ---
    parser.add_argument("--llm_load_in_8bit", action="store_true", help="Load LLM judge in 8-bit to save memory.")
    # ----------------------------------------

    args = parser.parse_args()

    # --- 动态导入评分函数 ---
    s2_extract_and_score, s4_show_scores = import_score_funcs(args.dataset_name)
    # -------------------------

    if args.score_with_llm:
        os.environ["USE_LLM_JUDGE"] = "1"
        os.environ["LLM_JUDGE_MODEL_PATH"] = args.llm_model_path
        # --- 【代码修改】: 处理 8bit 加载 ---
        # 如果指定了 8bit 加载，则忽略 dtype 设置，并设置 device_map 为 auto (bitsandbytes 需要)
        if args.llm_load_in_8bit:
            os.environ["LLM_JUDGE_LOAD_IN_8BIT"] = "1" # 需要 eval_utils.py 配合读取这个变量
            os.environ["LLM_JUDGE_DEVICE_MAP"] = "auto" # 8bit 需要 auto device map
            os.environ.pop("LLM_JUDGE_DTYPE", None) # 8bit 不能指定 dtype
            logger.info("LLM Judge will be loaded in 8-bit mode.")
        else:
            os.environ.pop("LLM_JUDGE_LOAD_IN_8BIT", None)
            os.environ["LLM_JUDGE_DEVICE_MAP"] = args.llm_device_map
            os.environ["LLM_JUDGE_DTYPE"] = args.llm_dtype
        # ------------------------------------
        os.environ["LLM_JUDGE_TEMPERATURE"] = str(args.llm_temperature)
        os.environ["LLM_JUDGE_MAX_NEW_TOKENS"] = str(args.llm_max_new_tokens)
        # --- 【代码修改】: 移除强制单进程的逻辑 ---
        # logger.warning("启用本地LLM判分后将改为单进程执行以避免重复加载模型。")
        # args.api_process = 1
        # ---------------------------------------
    else:
        os.environ.pop("USE_LLM_JUDGE", None)
        os.environ.pop("LLM_JUDGE_LOAD_IN_8BIT", None)

    args.api_process = max(1, args.api_process) # 确保至少有1个进程

    if args.config:
        model_output_dir = load_yaml(args.config)['output_dir']
    elif args.output_dir:
        model_output_dir = args.output_dir
    else:
        raise ValueError("Either --config or --output_dir must be provided.")
    print(f"Using output directory: {model_output_dir}")

    # --- 【代码修改】: 根据新参数控制执行流程 ---

    inference_dir = join(model_output_dir, 'inference', args.dataset_name)
    answers_file = join(inference_dir, f'answers_{args.tag}.json') # 定义最终合并文件名
    tmp_dir = join(inference_dir, 'tmp') # 定义临时文件目录
    os.makedirs(tmp_dir, exist_ok=True)

    if args.split_only > 0:
        logger.info(f"Splitting data into {args.split_only} chunks...")
        # Step 0: Merge raw inference outputs if needed (original s0)
        _ = s0_merge_answers(model_output_dir, args.dataset_name, args.tag) # Run s0 to ensure raw merged file exists
        # Step 1: Split the merged raw file
        _ = s1_separate_n(answers_file, args.split_only)
        logger.info(f"Data split into {args.split_only} chunks in {tmp_dir}. Exiting.")
        sys.exit(0) # 完成分割后退出

    elif args.process_chunk >= 0:
        chunk_id = args.process_chunk
        input_chunk_file = join(tmp_dir, f"{chunk_id}.json")
        output_chunk_file = args.output_chunk_file

        if not exists(input_chunk_file):
            logger.error(f"Input chunk file not found: {input_chunk_file}")
            sys.exit(1)
        if not output_chunk_file:
            logger.error("--output_chunk_file is required when using --process_chunk")
            sys.exit(1)
            
        # 确保输出目录存在
        os.makedirs(dirname(output_chunk_file), exist_ok=True)

        logger.info(f"Processing chunk {chunk_id}: {input_chunk_file} -> {output_chunk_file}")
        try:
            scored_data = _score_chunk_and_collect(s2_extract_and_score, input_chunk_file)
            save_json(output_chunk_file, scored_data)
            logger.info(f"Finished processing chunk {chunk_id}. Results saved to {output_chunk_file}")
        except Exception as e:
            logger.error(f"Error processing chunk {chunk_id}: {e}", exc_info=True)
            sys.exit(1)
        sys.exit(0) # 完成处理后退出

    elif args.merge_only:
        logger.info("Merging scored chunks and calculating final score...")
        # Step 3: Merge scored chunk files
        # 找到所有 scored_*.json 文件
        scored_files_pattern = join(tmp_dir, 'scored_*.json')
        scored_files_list = glob.glob(scored_files_pattern)
        if not scored_files_list:
             logger.error(f"No scored chunk files found matching '{scored_files_pattern}'. Cannot merge.")
             sys.exit(1)
             
        logger.info(f"Found {len(scored_files_list)} scored chunk files to merge.")
        
        # 使用 s3 合并这些评分过的文件到最终的 answers_file
        # **重要**: 确认 s3_merge_save_answers 能正确处理这些文件路径
        s3_merge_save_answers(answers_file, scored_files_list)
        logger.info(f"Merged scored results saved to {answers_file}")
        
        # Step 4: Show final scores from the merged scored file
        summary = s4_show_scores(answers_file)
        if summary:
            print(f"[Summary] Total Accuracy: {summary['overall_accuracy']:.3f} "
                  f"({summary['correct']}/{summary['total']})")
        logger.info("Final scores calculated. Exiting.")
        sys.exit(0) # 完成合并和评分后退出

    else:
        # --- 默认的完整流程 ---
        logger.info("Running the default full scoring pipeline...")
        # Step 0: Merge raw answers
        actual_answers_file = s0_merge_answers(model_output_dir, args.dataset_name, args.tag)
        # Step 1: Split raw answers
        answers_file_list = s1_separate_n(actual_answers_file, args.api_process)

        # Step 2: Process chunks (sequentially or parallel, depending on api_process and score_with_llm)
        scored_chunk_files = [] # 用于存储评分后文件的路径
        
        if args.api_process <= 1 or args.score_with_llm: # LLM judge 总是单进程处理（在此脚本内）
            logger.info("Processing chunks sequentially...")
            for i, chunk_file in enumerate(answers_file_list):
                output_file = join(tmp_dir, f"scored_{i}.json")
                try:
                    logger.info(f"Processing chunk {i}: {chunk_file} -> {output_file}")
                    scored_data = _score_chunk_and_collect(s2_extract_and_score, chunk_file)
                    save_json(output_file, scored_data)
                    scored_chunk_files.append(output_file)
                except Exception as e:
                    logger.error(f"Error processing chunk {i} ({chunk_file}): {e}", exc_info=True)
                    # Decide whether to continue or stop on error
        else: # 非LLM judge，可以使用多进程
            logger.info(f"Processing chunks in parallel with {args.api_process} workers...")
            job_map = {}
            with concurrent.futures.ProcessPoolExecutor(max_workers=args.api_process) as executor:
                for i, chunk_file in enumerate(answers_file_list):
                    future = executor.submit(_score_chunk_worker, args.dataset_name, chunk_file)
                    job_map[future] = (i, chunk_file)

                for future in concurrent.futures.as_completed(job_map):
                    i, chunk_file = job_map[future]
                    try:
                        processed_path = future.result()
                        scored_data = read_json(processed_path)
                        output_file = join(tmp_dir, f"scored_{i}.json")
                        save_json(output_file, scored_data)
                        scored_chunk_files.append(output_file)
                        logger.info(f"Finished processing chunk {i}: {chunk_file} -> {output_file}")
                    except Exception as e:
                        logger.error(f"Error processing chunk {i}: {e}", exc_info=True)

        # 确保所有块都已处理并有对应的输出文件
        if len(scored_chunk_files) != len(answers_file_list):
             logger.error("Mismatch in the number of processed chunks. Cannot merge.")
             sys.exit(1)

        # Step 3: Merge scored answers
        logger.info("Merging scored chunks...")
        s3_merge_save_answers(actual_answers_file, scored_chunk_files) # 合并到最终文件
        logger.info(f"Merged scored results saved to {actual_answers_file}")

        # Step 4: Show final scores
        s4_show_scores(actual_answers_file)
        logger.info("Full scoring pipeline finished.")
    # -----------------------------------------------

if __name__ == '__main__':
    scoring()
